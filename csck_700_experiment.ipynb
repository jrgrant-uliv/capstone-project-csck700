{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook document contains a series of experiments for machine learning and deep learning-based sequence analysis and sentiment analysis. The experiments include the following:\n",
    "\n",
    "1. Machine-learning based Sequence Analysis using a SVM (Support Vector Machine) model\n",
    "2. Deep-learning-based Sequence Analysis using a LSTM (Long Short-Term Memory) model\n",
    "3. Deep-learning-based Sequence Analysis using a DistilBERT (Transformer) model\n",
    "4. Deep-learning-based Sentiment Analysis using a TCN (Temporal Convolutional Network) model\n",
    "\n",
    "Each experiment focuses on analyzing sequences of events or text data using different models. The goal is to explore the performance and effectiveness of these models in various sequence analysis tasks.\n",
    "\n",
    "The code and explanations for each experiment are provided in the subsequent cells of this Jupyter Notebook document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n",
    "1. Environment\n",
    "2. Imports\n",
    "3. Globals\n",
    "4. Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and imports\n",
    "\n",
    "This section sets up the environment for the experiment and includes the necessary imports. It consists of the following subsections:\n",
    "\n",
    "Colab specific setup: This subsection contains the necessary installations and code checkout specific to Colab.\n",
    "Global variables and Settings: This subsection defines the global variables and settings used throughout the experiment.\n",
    "Utility Functions: This subsection includes utility functions that are used in the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup\n",
    "\n",
    "To ensure the smooth execution of this Jupyter Notebook document, it is important to perform the necessary Colab specific setup. This setup includes installing required packages, updating the base environment, and cloning the necessary codebase. By following these steps, you can ensure that the notebook runs seamlessly and all dependencies are properly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Specific installations and Code Checkout\n",
    "\n",
    "Here we check if the notebook is running on Google Colab. \n",
    "If it is, it we install the condacolab package, update conda and install important libraries using conda. \n",
    "We also clone the experiment's GitHub repository and copu the codebase and environment_setup directories if they don't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "is_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    !pip install condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()\n",
    "    !conda update -n base -c defaults conda\n",
    "    !conda install -y python=3.11 cudatoolkit tensorflow cudnn\n",
    "    !conda clean -ya\n",
    "\n",
    "    if not os.path.exists('codebase'):\n",
    "        !git clone https://github.com/jrgrant-uliv/capstone-project-csck700.git  \n",
    "        !cp -r /content/capstone-project-csck700/codebase ./\n",
    "        !cp -r /content/capstone-project-csck700/environment_setup ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resrouce Downloads\n",
    "\n",
    "This block is responsible for setting up the necessary resources and dependencies for the CSCK_700 experiment. \n",
    "\n",
    "It creates directories for embeddings and application log datasets. \n",
    "If the CSCK_700_Resources folder does not exist, it downloads it from a Google Drive link, unzips the supprting resources into their local directories, and then installs the python requrements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%mkdir -p artefacts/embeddings\n",
    "%mkdir -p application_log_datasets\n",
    "\n",
    "# if CSCK_700_Resources does not exist, download it\n",
    "if not os.path.exists('./CSCK_700_Resources'):\n",
    "    !pip install --upgrade gdown\n",
    "    !gdown https://drive.google.com/drive/folders/1Nsiyt_DseGU1tMTdb08AD65Y6puLIO9B -O ./ --folder\n",
    "    !unzip -oq ./CSCK_700_Resources/HDFS.zip -d ./application_log_datasets/HDFS\n",
    "    !unzip -oq ./CSCK_700_Resources/glove.840B.300d.zip -d ./artefacts/embeddings\n",
    "\n",
    "%cd environment_setup\n",
    "\n",
    "!sh install_dependencies.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the stock stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "from codebase.pipeline.preprocessors.preprocessor import (\n",
    "    BertEventTokenizer,\n",
    "    SequenceVectorizer,\n",
    "    SGTVectorizer,\n",
    ")\n",
    "from codebase.anomaly_detection.models import (\n",
    "    SVMClassifier,\n",
    "    LSTMAttentionClassifier,\n",
    "    TransformerClassifier,\n",
    "    TCNSentimentclassifier,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global varaibles and settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directories\n",
    "output_root = \"./output\"\n",
    "benchmark_results_dir = f\"{output_root}/benchmark_results\"\n",
    "benchmark_results_file = f\"{benchmark_results_dir}/benchmark_results.csv\"\n",
    "plot_dir = f\"{output_root}/plots\"\n",
    "plot_file = f\"{plot_dir}/benchmark_results.png\"\n",
    "model_dir = f\"{output_root}/models\"\n",
    "\n",
    "benchmark_results = []\n",
    "lstm_data = {\n",
    "    \"HDFS\": {\"train\": {\"X\": [], \"y\": []}, \"test\": {\"X\": [], \"y\": []}, \"loaded\": False},\n",
    "    \"Thunderbird\": {\n",
    "        \"train\": {\"X\": [], \"y\": []},\n",
    "        \"test\": {\"X\": [], \"y\": []},\n",
    "        \"loaded\": False,\n",
    "    },\n",
    "}\n",
    "svm_data = {\n",
    "    \"HDFS\": {\"train\": {\"X\": [], \"y\": []}, \"test\": {\"X\": [], \"y\": []}, \"loaded\": False},\n",
    "    \"Thunderbird\": {\n",
    "        \"train\": {\"X\": [], \"y\": []},\n",
    "        \"test\": {\"X\": [], \"y\": []},\n",
    "        \"loaded\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "data_sets = {\n",
    "    \"HDFS\": {\n",
    "        # The benchmark dataset\n",
    "        \"struct_log\": \"./application_log_datasets/HDFS/HDFS.event_traces.csv\",\n",
    "        # The event template file\n",
    "        \"template_file\": \"./application_log_datasets/HDFS/HDFS.log_templates.csv\",\n",
    "    },\n",
    "    \"Thunderbird\": {\n",
    "        # The benchmark dataset\n",
    "        \"struct_log\": \"application_log_datasets/Thunderbird/Thunderbird_20M.log_structured.csv\",\n",
    "        # The event template file\n",
    "        \"template_file\": \"application_log_datasets/Thunderbird/Thunderbird_20M.log_templates.csv\",\n",
    "    },\n",
    "}\n",
    "print(\"Data sets loaded\")\n",
    "# print(data_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure output directories exist\n",
    "if not os.path.exists(output_root):\n",
    "    os.mkdir(output_root)\n",
    "if not os.path.exists(benchmark_results_dir):\n",
    "    os.mkdir(benchmark_results_dir)\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.mkdir(plot_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "\n",
    "def load_event_templates_hdfs(event_templates):\n",
    "    \"\"\"\n",
    "    Load event templates from a CSV file and create a dictionary mapping event IDs to event texts.\n",
    "\n",
    "    Parameters:\n",
    "    event_templates (str): The path to the CSV file containing event templates.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping event IDs to event texts.\n",
    "    \"\"\"\n",
    "    df_event_templates = pd.read_csv(event_templates)\n",
    "    # create an event_id to event_text dictionary\n",
    "    event_id_to_event_text = {}\n",
    "    for index, row in df_event_templates.iterrows():\n",
    "        event_id_to_event_text[row[\"EventId\"]] = row[\"EventTemplate\"]\n",
    "    return event_id_to_event_text\n",
    "\n",
    "\n",
    "def load_data_hdfs(event_traces, event_templates):\n",
    "    \"\"\"\n",
    "    Load data from HDFS and perform feature and label extraction.\n",
    "\n",
    "    Parameters:\n",
    "    event_traces (str): Path to the CSV file containing event traces.\n",
    "    event_templates (str): Path to the CSV file containing event templates.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the feature array (x) and the label array (y).\n",
    "    \"\"\"\n",
    "\n",
    "    df_event_traces = pd.read_csv(event_traces)\n",
    "\n",
    "    # Label: Success = 0, rest = 1\n",
    "    df_event_traces[\"Label\"] = df_event_traces[\"Label\"].apply(\n",
    "        lambda x: 0 if x == \"Success\" else 1\n",
    "    )\n",
    "\n",
    "    # feature and label extraction\n",
    "    x = df_event_traces[\"Features\"].values\n",
    "    y = df_event_traces[\"Label\"].values\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_data(data_set, model, validation_data=False):\n",
    "    \"\"\"\n",
    "    Load the benchmark dataset.\n",
    "\n",
    "    Args:\n",
    "        data_set (str): The name of the benchmark dataset.\n",
    "        window_size (int): The size of the sliding window.\n",
    "        train_ratio (float): The ratio of the training set to the entire dataset.\n",
    "        split_type (str): The type of the splitting method. It can be 'uniform' or 'sequential'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training set and test set.\n",
    "    \"\"\"\n",
    "    log_file = data_sets[data_set][\"struct_log\"]\n",
    "    if \"label_file\" in data_sets[data_set]:\n",
    "        label_file = data_sets[data_set][\"label_file\"]\n",
    "    # load templates if in data_sets[data_set]\n",
    "    template_file = None\n",
    "    if \"template_file\" in data_sets[data_set]:\n",
    "        template_file = data_sets[data_set][\"template_file\"]\n",
    "\n",
    "        x, y = load_data_hdfs(log_file, template_file)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparaion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hdfs_data(x, y):\n",
    "    \"\"\"\n",
    "    Prepares the HDFS data for training by generating augmented data and combining it with the original data.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): The feature data.\n",
    "        y (numpy.ndarray): The label data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The prepared data with augmented samples.\n",
    "\n",
    "    \"\"\"\n",
    "    label_counts = np.bincount(y)\n",
    "    pos_count = label_counts[1]\n",
    "    augmenation_cap = int(pos_count * 0.75)\n",
    "    print(\"Augmentation cap: \", augmenation_cap)\n",
    "    print(\"Label counts: \", label_counts)\n",
    "\n",
    "    new_abnormal = generate_augmented_data(x, augmenation_cap)\n",
    "\n",
    "    original_data_df = pd.DataFrame({\"feature\": x, \"label\": y})\n",
    "    new_abnormal_df = pd.DataFrame({\"feature\": new_abnormal, \"label\": 1})\n",
    "\n",
    "    data_df = pd.concat([original_data_df, new_abnormal_df], ignore_index=True)\n",
    "    # shuffle data_df\n",
    "    data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def generate_augmented_data(normal_data, augmentation_sample_size):\n",
    "    \"\"\"\n",
    "    Generate augmented data by applying various anomaly generation techniques to the given normal data.\n",
    "\n",
    "    Parameters:\n",
    "    normal_data (list): A list of strings representing the normal data.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings representing the augmented data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    random_state = 42\n",
    "    random.seed(random_state)\n",
    "\n",
    "    unique_sequence_ids = []\n",
    "    augment_data = []\n",
    "    for x in normal_data:\n",
    "        x = x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "        # of the sequence is longer than 5 and has more than 3 unique sequence ids, add it to the augmented data\n",
    "        if len(x) > 5 and len(set(x)) > 3:\n",
    "            augment_data.append(x)\n",
    "            if len(augment_data) >= augmentation_sample_size:\n",
    "                break\n",
    "        for y in x:\n",
    "            if y not in unique_sequence_ids:\n",
    "                unique_sequence_ids.append(y)\n",
    "    print(\"Number of unique sequence ids: \", len(unique_sequence_ids))\n",
    "    print(\"Size of sample for augmentation: \", len(augment_data))\n",
    "    print(\"Sampled data: \", augment_data[0])\n",
    "    sampled_data = random.sample(augment_data, int(len(augment_data) * 0.6))\n",
    "    reversed_sequences = []\n",
    "    for x in sampled_data:\n",
    "        x.reverse()\n",
    "        reversed_sequences.append(x)\n",
    "    print(\"Sampled reversed sequences: \", reversed_sequences[:5])\n",
    "    # randomly select another 20% of the normal dataset and generate shuffled sequences as anomalies\n",
    "    sampled_data = random.sample(augment_data, int(len(augment_data) * 0.3))\n",
    "    shuffled_sequences = []\n",
    "    for x in sampled_data:\n",
    "        random.shuffle(x)\n",
    "        shuffled_sequences.append(x)\n",
    "    print(\"Sampled shuffled sequences: \", shuffled_sequences[:5])\n",
    "\n",
    "    # randomly select another 20% of the normal dataset and randomly insert sequence ids from unique_sequence_ids as anomalies\n",
    "    sampled_data = random.sample(augment_data, int(len(augment_data) * 0.2))\n",
    "    inserted_sequences = []\n",
    "    for x in sampled_data:\n",
    "        # insert a random sequence id from unique_sequence_ids at a random point\n",
    "        random_index = random.randint(0, len(x) - 1)\n",
    "        # insert up to 10 sequence ids\n",
    "        insert_count = random.randint(1, 10)\n",
    "        for i in range(insert_count):\n",
    "            random_sequence_id = random.choice(unique_sequence_ids)\n",
    "            x.insert(random_index, random_sequence_id)\n",
    "        inserted_sequences.append(x)\n",
    "    print(\"Sampled inserted sequences: \", inserted_sequences[:5])\n",
    "    # combine all the augmented data\n",
    "    augmented_data = reversed_sequences + shuffled_sequences + inserted_sequences\n",
    "    print(\"Number of augmented sequences: \", len(augmented_data))\n",
    "    print(\"Sampled augmented data: \", augmented_data[:5])\n",
    "    # reassemble as a string representation of a list\n",
    "    new_abnormal = []\n",
    "    for lst in augment_data:\n",
    "        lst = \",\".join(lst)\n",
    "        lst = f\"[{lst}]\"\n",
    "        new_abnormal.append(lst)\n",
    "    return new_abnormal\n",
    "\n",
    "\n",
    "def process_text_corpus(text_corpus_df, word_split=None):\n",
    "    \"\"\"\n",
    "    Process the text corpus dataframe by performing various transformations.\n",
    "\n",
    "    Args:\n",
    "        text_corpus_df (pandas.DataFrame): The input text corpus dataframe.\n",
    "        word_split (dict, optional): A dictionary containing words to split and their replacements.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed text corpus dataframe.\n",
    "    \"\"\"\n",
    "    split_words = word_split is not None\n",
    "    lm = WordNetLemmatizer()\n",
    "    english_stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Train Corpus\n",
    "    print(\"Process Training Corpus\")\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].replace(\n",
    "        {\"<.*?>\": \" \"}, regex=True\n",
    "    )\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].replace(\n",
    "        {\"[^a-zA-Z]\": \" \"}, regex=True\n",
    "    )\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].replace(\n",
    "        {\"\\s+\": \" \"}, regex=True\n",
    "    )\n",
    "    if split_words:\n",
    "        for word in word_split:\n",
    "            replace = \" \".join(word_split[word]).lower()\n",
    "            print(replace)\n",
    "            text_corpus_df[\"EventTemplate\"] = text_corpus_df[\n",
    "                \"EventTemplate\"\n",
    "            ].str.replace(word, replace)\n",
    "\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].apply(\n",
    "        lambda x: [\n",
    "            lm.lemmatize(word)\n",
    "            for word in x.split(\" \")\n",
    "            if not word in english_stops and word != \"\"\n",
    "        ]\n",
    "    )\n",
    "    # remove '' from list in column X\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].apply(\n",
    "        lambda x: [word for word in x if word != \"\"]\n",
    "    )\n",
    "    # convert list in column X to string\n",
    "    text_corpus_df[\"EventTemplate\"] = text_corpus_df[\"EventTemplate\"].apply(\n",
    "        lambda x: \" \".join(x)\n",
    "    )\n",
    "\n",
    "    return text_corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate positional encoding for transformer models.\n",
    "\n",
    "    Parameters:\n",
    "    - max_len (int): Maximum sequence length.\n",
    "    - d_model (int): Dimensionality of the model.\n",
    "\n",
    "    Returns:\n",
    "    - pos_enc (np.ndarray): Positional encoding of shape (1, max_len, d_model).\n",
    "    \"\"\"\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pos_enc = np.zeros((max_len, d_model))\n",
    "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "    pos_enc = pos_enc[np.newaxis, ...]\n",
    "    return pos_enc.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_deployment(classifier, vectorizer):\n",
    "    \"\"\"\n",
    "    Saves the trained classifier model and associated artifacts for deployment.\n",
    "\n",
    "    Args:\n",
    "        classifier (Classifier): The trained classifier object.\n",
    "        vectorizer (Vectorizer): The vectorizer object used for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    classifier.save_model_file()\n",
    "    model_artefact_dir = classifier.model_artefact_dir\n",
    "    if vectorizer is not None:\n",
    "        tokenizer_file = os.path.join(model_artefact_dir, \"tokenizer.pkl\")\n",
    "        vectorizer.save_tokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    clf,\n",
    "    _model,\n",
    "    _data_set,\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a machine learning model.\n",
    "\n",
    "    Args:\n",
    "        clf: The classifier model.\n",
    "        _model: The name of the model.\n",
    "        _data_set: The name of the dataset.\n",
    "        accuracies: List of accuracy scores.\n",
    "        precisions: List of precision scores.\n",
    "        recalls: List of recall scores.\n",
    "        fscores: List of F1 scores.\n",
    "        aucs: List of AUC scores.\n",
    "        conf_matrices: List of confusion matrices.\n",
    "        roc_curves: List of ROC curves.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conf_matrix = np.mean(conf_matrices, axis=0).astype(np.int32)\n",
    "    tn = conf_matrix[0][0]\n",
    "    fp = conf_matrix[0][1]\n",
    "    tp = conf_matrix[1][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    pos = tp + fn\n",
    "    neg = fp + tn\n",
    "    accuracy = np.mean(accuracies)\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(fscores)\n",
    "    auc = np.mean(aucs)\n",
    "    TPR = tp / pos\n",
    "    FPR = fp / neg\n",
    "    TNR = tn / neg\n",
    "    FNR = fn / pos\n",
    "    CSR = (tp + tn) / (pos + neg)\n",
    "    CFR = (fp + fn) / (pos + neg)\n",
    "    MTTD_Impact = (FPR * 2) + FNR\n",
    "\n",
    "    save_conf_matrix(clf, conf_matrix)\n",
    "    plot_confusion_matrix(clf, conf_matrix)\n",
    "    plot_roc(clf, roc_curves, aucs)\n",
    "\n",
    "    save_benchmark_results(\n",
    "        _model,\n",
    "        _data_set,\n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "        auc,\n",
    "        conf_matrix,\n",
    "        tp,\n",
    "        fp,\n",
    "        tn,\n",
    "        fn,\n",
    "        pos,\n",
    "        neg,\n",
    "        TPR,\n",
    "        FPR,\n",
    "        TNR,\n",
    "        FNR,\n",
    "        CSR,\n",
    "        CFR,\n",
    "        MTTD_Impact\n",
    "    )\n",
    "\n",
    "    plot_metrics()\n",
    "\n",
    "\n",
    "def evaluate_svm(clf, _model, _data_set, y_pred, y_true, accuracy):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a Support Vector Machine (SVM) classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - clf: The trained SVM classifier.\n",
    "    - _model: The name or identifier of the model being evaluated.\n",
    "    - _data_set: The name or identifier of the dataset being evaluated.\n",
    "    - y_pred: The predicted labels.\n",
    "    - y_true: The true labels.\n",
    "    - accuracy: The accuracy scores for each prediction.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.mean(accuracy)\n",
    "\n",
    "    # Calculate precision, recall, and F-score\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    tp = conf_matrix[0][0]\n",
    "    fp = conf_matrix[0][1]\n",
    "    tn = conf_matrix[1][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    pos = tp + fn\n",
    "    neg = fp + tn\n",
    "    TPR = tp / pos\n",
    "    FPR = fp / neg\n",
    "    TNR = tn / neg\n",
    "    FNR = fn / pos\n",
    "    CSR = (tp + tn) / (pos + neg)\n",
    "    CFR = (fp + fn) / (pos + neg)\n",
    "    MTTD_Impact = (FPR * 2) + FNR\n",
    "\n",
    "\n",
    "    save_conf_matrix(clf, conf_matrix)\n",
    "    plot_confusion_matrix(clf, conf_matrix)\n",
    "    roc = roc_curve(y_true, y_pred)\n",
    "    plot_roc(clf, [roc], [auc])\n",
    "\n",
    "    save_benchmark_results(\n",
    "        _model,\n",
    "        _data_set,\n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "        auc,\n",
    "        conf_matrix,\n",
    "        tp,\n",
    "        fp,\n",
    "        tn,\n",
    "        fn,\n",
    "        pos,\n",
    "        neg,\n",
    "        TPR,\n",
    "        FPR,\n",
    "        TNR,\n",
    "        FNR,\n",
    "        CSR,\n",
    "        CFR,\n",
    "        MTTD_Impact,\n",
    "    )\n",
    "\n",
    "    plot_metrics()\n",
    "\n",
    "\n",
    "def save_conf_matrix(clf, conf_matrix):\n",
    "    \"\"\"\n",
    "    Save the confusion matrix to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        clf (object): The classifier object.\n",
    "        conf_matrix (array-like): The confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model_conf_matrix_file = os.path.join(\n",
    "        clf.model_artefact_dir, \"confusion_matrix.csv\"\n",
    "    )\n",
    "    conf_matrix_df = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "        columns=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "    )\n",
    "    conf_matrix_df.to_csv(model_conf_matrix_file)\n",
    "\n",
    "\n",
    "def save_benchmark_results(\n",
    "    model,\n",
    "    dataset,\n",
    "    accuracy,\n",
    "    precision,\n",
    "    recall,\n",
    "    f1,\n",
    "    auc,\n",
    "    conf_matrix,\n",
    "    tp,\n",
    "    fp,\n",
    "    tn,\n",
    "    fn,\n",
    "    pos,\n",
    "    neg,\n",
    "    TPR,\n",
    "    FPR,\n",
    "    TNR,\n",
    "    FNR,\n",
    "    CSR,\n",
    "    CFR,\n",
    "    MTTD_Impact\n",
    "):\n",
    "    \"\"\"\n",
    "    Save benchmark results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model.\n",
    "        dataset (str): The name of the dataset.\n",
    "        accuracy (float): The accuracy score.\n",
    "        precision (float): The precision score.\n",
    "        recall (float): The recall score.\n",
    "        f1 (float): The F1 score.\n",
    "        auc (float): The AUC score.\n",
    "        conf_matrix (array-like): The confusion matrix.\n",
    "        tp (int): The number of true positives.\n",
    "        fp (int): The number of false positives.\n",
    "        tn (int): The number of true negatives.\n",
    "        fn (int): The number of false negatives.\n",
    "        pos (int): The number of positive instances.\n",
    "        neg (int): The number of negative instances.\n",
    "        mttd_impact (float): The impact score 1.\n",
    "    \"\"\"\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"AUC: \", auc)\n",
    "    print(\"Confusion Matrix: \", conf_matrix)\n",
    "\n",
    "    # add metrics to extended_benchmark_results\n",
    "    benchmark_results.append(\n",
    "        [\n",
    "            model,\n",
    "            dataset,\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1,\n",
    "            auc,\n",
    "            pos,\n",
    "            neg,\n",
    "            tp,\n",
    "            fp,\n",
    "            fn,\n",
    "            tn,\n",
    "            TPR,\n",
    "            FPR,\n",
    "            TNR,\n",
    "            FNR,\n",
    "            CSR,\n",
    "            CFR,\n",
    "            MTTD_Impact,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(benchmark_results_dir):\n",
    "        os.makedirs(benchmark_results_dir)\n",
    "    df = pd.DataFrame(\n",
    "        benchmark_results,\n",
    "        columns=[\n",
    "            \"model\",\n",
    "            \"dataset\",\n",
    "            \"accuracy\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\",\n",
    "            \"auc\",\n",
    "            \"pos\",\n",
    "            \"neg\",\n",
    "            \"tp\",\n",
    "            \"fp\",\n",
    "            \"fn\",\n",
    "            \"tn\",\n",
    "            \"tpr\",\n",
    "            \"fpr\",\n",
    "            \"tnr\",\n",
    "            \"fnr\",\n",
    "            \"csr\",\n",
    "            \"cfr\",\n",
    "            \"mttd_impact\",\n",
    "        ],\n",
    "    )\n",
    "    df.to_csv(benchmark_results_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process Glove Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Downloads and processes the GloVe embeddings if necessary.\n",
    "\n",
    "Args:\n",
    "    None\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "path = './artefacts/embeddings/'\n",
    "nb_file_path=path+'glove.840B.300d.txt'\n",
    "pkl_file = f'{path}/glove.840B.300d.pkl'\n",
    "\n",
    "force_process_glove = False\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    force_process_glove = True\n",
    "\n",
    "if not os.path.exists(pkl_file):\n",
    "    force_process_glove = True\n",
    "\n",
    "if force_process_glove:\n",
    "    if not os.path.exists(nb_file_path):\n",
    "        !wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P {path}\n",
    "        !unzip {path}/glove.840B.300d.zip -d {path}\n",
    "        !rm {path}/glove.840B.300d.zip\n",
    "\n",
    "    df = pd.read_csv(nb_file_path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    embeddings_index = {key: val.values for key, val in df.T.items()}\n",
    "    #\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    import pickle\n",
    "    pkl_file = f'{path}/glove.840B.300d.pkl'\n",
    "    with open(pkl_file, 'wb') as fp:\n",
    "        pickle.dump(embeddings_index, fp)\n",
    "else:\n",
    "    reload_embedding_index = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics():\n",
    "    \"\"\"\n",
    "    Plots the metrics (accuracy, precision, recall, and F1 score) for different models and datasets.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        benchmark_results,\n",
    "        columns=[\n",
    "            \"model\",\n",
    "            \"dataset\",\n",
    "            \"accuracy\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"f1\",\n",
    "            \"auc\",\n",
    "            \"pos\",\n",
    "            \"neg\",\n",
    "            \"tp\",\n",
    "            \"fp\",\n",
    "            \"fn\",\n",
    "            \"tn\",\n",
    "            \"tpr\",\n",
    "            \"fpr\",\n",
    "            \"tnr\",\n",
    "            \"fnr\",\n",
    "            \"csr\",\n",
    "            \"cfr\",\n",
    "            \"mttd_impact\",\n",
    "        ],\n",
    "    )\n",
    "    # Define a color palette for models\n",
    "    model_palette = sns.color_palette(\"Set1\", n_colors=len(df['model'].unique()))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x=\"model\", y=\"accuracy\", hue=\"dataset\", data=df, palette=model_palette)\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    # Precision Plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x=\"model\", y=\"precision\", hue=\"dataset\", data=df, palette=model_palette)\n",
    "    plt.title(\"Precision\")\n",
    "\n",
    "    # Recall Plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x=\"model\", y=\"recall\", hue=\"dataset\", data=df, palette=model_palette)\n",
    "    plt.title(\"Recall\")\n",
    "\n",
    "    # F1 Score Plot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x=\"model\", y=\"f1\", hue=\"dataset\", data=df, palette=model_palette)\n",
    "    plt.title(\"F1 Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(plot_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc(clf, roc_curves, auc_scores):\n",
    "    \"\"\"\n",
    "    Plots the ROC curves for a classifier.\n",
    "\n",
    "    Args:\n",
    "        clf (Classifier): The classifier object.\n",
    "        roc_curves (list): List of ROC curves.\n",
    "        auc_score (float): The AUC score.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model_name = clf.model_name\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    split = 1\n",
    "    for roc in roc_curves:\n",
    "        fpr = roc[0]\n",
    "        tpr = roc[1]\n",
    "        auc = auc_scores[split - 1]\n",
    "        plt.plot(\n",
    "            fpr, tpr, label=f\"Cross-validation split {split} - AUC = {auc:.2f}\"\n",
    "        )\n",
    "        split += 1\n",
    "\n",
    "    plt.title(f\"ROC Curves {model_name}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(clf.model_artefact_dir, \"roc_curves.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(clf, conf_matrix):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix using a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - clf: The classifier object.\n",
    "    - conf_matrix: The confusion matrix to be plotted.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"g\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"True\", \"False\"],\n",
    "        yticklabels=[\"True\", \"False\"],\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix - \" + clf.model_name)\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.savefig(os.path.join(clf.model_artefact_dir, \"confusion_matrix.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads HDFS event sequences, prepares the data, and optionally reduces the data.\n",
    "\"\"\"\n",
    "\n",
    "reduce_data = False\n",
    "print(\"Loading HDFS Event Sequences\")\n",
    "x, y = load_data(\"HDFS\", \"SVM\")\n",
    "hdfs_data_df = prepare_hdfs_data(x, y)\n",
    "\n",
    "if reduce_data:\n",
    "    print(\"Reducing HDFS Event Sequences\")\n",
    "    hdfs_data_df = hdfs_data_df.drop(\n",
    "        hdfs_data_df[hdfs_data_df.label == 0].sample(\n",
    "            frac=0.5, random_state=42).index\n",
    "    )\n",
    "    hdfs_data_df = hdfs_data_df.drop(\n",
    "        hdfs_data_df[hdfs_data_df.label == 1].sample(\n",
    "            frac=0.5, random_state=42).index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"Num GPUs Available: \", len(\n",
    "    tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Running in Colab: \", is_colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run all the cells above this one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - SVM\n",
    "\n",
    "Statistical Analysis using an SVM (Support Vector Machine) model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs an experiment using the HDFS dataset and SVM model.\n",
    "It loads the data, prepares it, and splits it into train and test sets.\n",
    "It then applies TF-IDF vectorization to the data and calculates class weights.\n",
    "Finally, it prints the shapes of the train and test data, the normal to anomaly ratio,\n",
    "the counts of normal and anomaly classes, and the class weights.\n",
    "\"\"\"\n",
    "\n",
    "_data_set = \"HDFS\"\n",
    "_model = \"SVM\"\n",
    "print(_data_set, _model)\n",
    "\n",
    "if hdfs_data_df is None:\n",
    "    x, y = load_data(_data_set, _model)\n",
    "    hdfs_data_df = prepare_hdfs_data(x, y)\n",
    "\n",
    "x = hdfs_data_df[\"feature\"].tolist()\n",
    "y = hdfs_data_df[\"label\"].tolist()\n",
    "y = np.array(y)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = SequenceVectorizer(mode=\"tfidf\")\n",
    "vectorizer.fit(x)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=42\n",
    ")\n",
    "train_data = vectorizer.transform(train_data)\n",
    "test_data = vectorizer.transform(test_data)\n",
    "\n",
    "class_counts = np.unique(train_labels, return_counts=True)[1]\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Normal to anomaly ratio: \", ratio)\n",
    "print(\"Normal: \", class_counts[0])\n",
    "print(\"Anomaly: \", class_counts[1])\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs cross-validation using an SVM classifier model.\n",
    "It sets the batch size, LSTM units, number of epochs, number of splits, prediction threshold, and model name.\n",
    "Then it initializes an SVMClassifier object with the specified model name and class weights.\n",
    "Next, it builds the SVM model using the SVMClassifier object.\n",
    "After that, it performs cross-validation using the built model and obtains predictions.\n",
    "Finally, it calculates the accuracy of the model using cross-validation scores and evaluates the SVM classifier.\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "lstm_units = 8\n",
    "num_epochs = 10\n",
    "num_splits = 5\n",
    "prediction_threshold = 0.9\n",
    "model_name = \"SVMClassifier_HDFS\"\n",
    "\n",
    "classifier = SVMClassifier(model_name=model_name, class_weight=class_weights)\n",
    "svc_model = classifier.build_model()\n",
    "\n",
    "# Perform cross-validation and get predictions\n",
    "y_pred = cross_val_predict(svc_model, train_data, train_labels, cv=5)\n",
    "# Calculate accuracy\n",
    "accuracy = cross_val_score(\n",
    "    svc_model, train_data, train_labels, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluate_svm(classifier, _model, _data_set, y_pred, train_labels, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1. Sequence Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code prepares the data for training a model on the HDFS dataset using LSTM.\n",
    "It loads the data, prepares it for training, and calculates class weights for imbalanced data.\n",
    "\"\"\"\n",
    "\n",
    "_data_set = \"HDFS\"\n",
    "_model = \"LSTM\"\n",
    "print(_data_set, _model)\n",
    "\n",
    "sequence_length = 128\n",
    "feature_dim = 1\n",
    "\n",
    "if hdfs_data_df is None:\n",
    "    x, y = load_data(_data_set, _model)\n",
    "    hdfs_data_df = prepare_hdfs_data(x, y)\n",
    "\n",
    "\n",
    "x = hdfs_data_df[\"feature\"].tolist()\n",
    "y = hdfs_data_df[\"label\"].tolist()\n",
    "y = np.array(y)\n",
    "\n",
    "vectorizer = SequenceVectorizer(num_words=sequence_length)\n",
    "vectorizer.fit(x)\n",
    "train_data = vectorizer.transform(x)\n",
    "train_labels = y\n",
    "\n",
    "vocab_size = len(vectorizer.tokenizer.word_index) + 1\n",
    "train_data = train_data.reshape(\n",
    "    train_data.shape[0], sequence_length, feature_dim)\n",
    "\n",
    "class_counts = np.unique(train_labels, return_counts=True)[1]\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Normal to anomaly ratio: \", ratio)\n",
    "print(\"Normal: \", class_counts[0])\n",
    "print(\"Anomaly: \", class_counts[1])\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs cross-validation and evaluation of an LSTMAttentionClassifier model.\n",
    "It sets the batch size, LSTM units, number of epochs, number of splits, prediction threshold, and model name.\n",
    "Then it initializes an LSTMAttentionClassifier object with the specified parameters.\n",
    "Next, it performs cross-validation using the cross_validate method of the classifier object.\n",
    "Finally, it evaluates the model using the evaluate_model function, passing in the classifier object and various evaluation metrics.\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "lstm_units = 8\n",
    "num_epochs = 10\n",
    "num_splits = 5\n",
    "prediction_threshold = 0.9\n",
    "model_name = \"LSTMAttentionClassifier_HDFS_SequenceMatrix\"\n",
    "\n",
    "classifier = LSTMAttentionClassifier(\n",
    "    model_name,\n",
    "    feature_dim,\n",
    "    sequence_length,\n",
    "    vocab_size,\n",
    "    lstm_units=lstm_units,\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ") = classifier.cross_validate(\n",
    "    train_data, train_labels, num_splits, num_epochs, prediction_threshold\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    classifier,\n",
    "    _model,\n",
    "    _data_set,\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2. Sequence Embeddings usint Sequence Graph Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code prepares and processes the HDFS dataset for training a model. It performs the following steps:\n",
    "\n",
    "1. Loads and prepares the HDFS data if it is not already loaded.\n",
    "2. Converts the feature and label columns into lists.\n",
    "3. Prepares the train data by creating a DataFrame with sequence and label columns.\n",
    "4. Applies sequence-to-graph transformation using SGTVectorizer.\n",
    "5. Converts the train data to arrays.\n",
    "6. Calculates class weights based on the ratio of normal to anomaly labels.\n",
    "7. Prints information about the data, including train data shape, normal to anomaly ratio, class counts, and class weights.\n",
    "\"\"\"\n",
    "\n",
    "_data_set = \"HDFS\"\n",
    "_model = \"LSTM\"\n",
    "print(_data_set, _model)\n",
    "\n",
    "sequence_length = 32\n",
    "feature_dim = 1\n",
    "\n",
    "# Load and prepare the HDFS data\n",
    "if hdfs_data_df is None:\n",
    "    x, y = load_data(_data_set, _model)\n",
    "    hdfs_data_df = prepare_hdfs_data(x, y)\n",
    "\n",
    "x = hdfs_data_df[\"feature\"].tolist()\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i].replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \").split(\" \")\n",
    "\n",
    "y = hdfs_data_df[\"label\"].tolist()\n",
    "y = np.array(y)\n",
    "\n",
    "# Prepare train data\n",
    "train_data_df = pd.DataFrame({\"sequence\": x, \"label\": y})\n",
    "train_data_df = train_data_df.reset_index()\n",
    "train_data_df = train_data_df.rename(columns={\"index\": \"id\"})\n",
    "train_labels = train_data_df[\"label\"].tolist()\n",
    "train_data_df.drop(columns=[\"label\"], inplace=True)\n",
    "\n",
    "# Apply sequence-to-graph transformation using SGTVectorizer\n",
    "vectorizer = SGTVectorizer(num_dims=sequence_length)\n",
    "train_data = vectorizer.fit_transform(train_data_df)\n",
    "\n",
    "# Convert train and test data to arrays\n",
    "train_data_arr = []\n",
    "for i in train_data:\n",
    "    i = i.tolist()\n",
    "    train_data_arr.append(i)\n",
    "train_data.shape\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.unique(train_labels, return_counts=True)[1]\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "\n",
    "# Print information about the data\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Normal to anomaly ratio: \", ratio)\n",
    "print(\"Normal: \", class_counts[0])\n",
    "print(\"Anomaly: \", class_counts[1])\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs cross-validation and evaluation of an LSTMAttentionClassifier model.\n",
    "It sets the values for various parameters such as vocab_size, batch_size, lstm_units, num_epochs, num_splits, and prediction_threshold.\n",
    "The model is initialized with the specified parameters and then cross-validated using the cross_validate() method.\n",
    "The resulting accuracies, precisions, recalls, fscores, aucs, conf_matrices, and roc_curves are then used to evaluate the model using the evaluate_model() function.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 500\n",
    "batch_size = 128\n",
    "lstm_units = 8\n",
    "num_epochs = 10\n",
    "num_splits = 5\n",
    "prediction_threshold = 0.9\n",
    "model_name = \"LSTMAttentionClassifier_HDFS_SGT\"\n",
    "\n",
    "classifier = LSTMAttentionClassifier(\n",
    "    model_name,\n",
    "    feature_dim,\n",
    "    sequence_length,\n",
    "    vocab_size,\n",
    "    lstm_units=lstm_units,\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ") = classifier.cross_validate(\n",
    "    train_data, train_labels, num_splits, num_epochs, prediction_threshold\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    classifier,\n",
    "    _model,\n",
    "    _data_set,\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preaparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "This code performs data preprocessing and sets up the necessary variables for training a Transformer model on the HDFS dataset.\n",
    "\"\"\"\n",
    "\n",
    "_data_set = \"HDFS\"\n",
    "_model = \"Transformer\"\n",
    "print(_data_set, _model)\n",
    "batch_size = 32\n",
    "\n",
    "if hdfs_data_df is None:\n",
    "    x, y = load_data(_data_set, _model)\n",
    "    hdfs_data_df = prepare_hdfs_data(x, y)\n",
    "\n",
    "reduce_data_df = hdfs_data_df.drop(\n",
    "    hdfs_data_df[hdfs_data_df.label == 0].sample(\n",
    "        frac=0.75, random_state=42).index\n",
    ")\n",
    "\n",
    "x = reduce_data_df[\"feature\"].tolist()\n",
    "y = reduce_data_df[\"label\"].tolist()\n",
    "y = np.array(y)\n",
    "\n",
    "max_len = max([len(i) for i in x])\n",
    "\n",
    "model_type = \"distilbert-base-uncased\"\n",
    "preprocessor = BertEventTokenizer(\n",
    "    model_type=model_type, batch_size=batch_size, max_length=128\n",
    ")\n",
    "train_tokens, train_masks = preprocessor.transform(x)\n",
    "vocab_size = preprocessor.vocab_size\n",
    "seq_len = preprocessor.max_length\n",
    "train_data = [train_tokens, train_masks]\n",
    "train_labels = y\n",
    "\n",
    "class_counts = np.unique(train_labels, return_counts=True)[1]\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "\n",
    "print(\"Train data shape: \", train_data[0].shape)\n",
    "print(\"Normal to anomaly ratio: \", ratio)\n",
    "print(\"Normal: \", class_counts[0])\n",
    "print(\"Anomaly: \", class_counts[1])\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs a cross-validation experiment using a TransformerClassifier model.\n",
    "It sets the sequence length, batch size, number of epochs, number of splits, prediction threshold, and model name.\n",
    "Then it initializes a TransformerClassifier object with the specified parameters.\n",
    "Next, it calls the `cross_validate` method of the classifier object to perform cross-validation on the training data.\n",
    "The results of the cross-validation are stored in the variables `accuracies`, `precisions`, `recalls`, `fscores`, `aucs`, `conf_matrices`, and `roc_curves`.\n",
    "Finally, it calls the `evaluate_model` function to evaluate the model using the obtained results.\n",
    "\"\"\"\n",
    "\n",
    "seq_len = 640\n",
    "batch_size = 16\n",
    "if is_colab:\n",
    "    batch_size = 64\n",
    "num_epochs = 10\n",
    "num_splits = 5\n",
    "prediction_threshold = 0.9\n",
    "model_name = \"Transformer_classifier_HDFS\"\n",
    "\n",
    "classifier = TransformerClassifier(\n",
    "    \"Transformer_classifier_HDFS\", 1, seq_len, vocab_size, model_type=model_type\n",
    ")\n",
    "\n",
    "(\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ") = classifier.cross_validate(\n",
    "    train_data, train_labels, num_splits, num_epochs, prediction_threshold\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    classifier,\n",
    "    _model,\n",
    "    _data_set,\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - TCN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This code performs data preprocessing and prepares the data for training a model on the HDFS dataset.\n",
    "It loads event templates, processes text corpus, applies feature string mapping, and prepares embedding matrix.\n",
    "It also calculates class weights for imbalanced data and prints relevant information about the data.\n",
    "\"\"\"\n",
    "\n",
    "_data_set = \"HDFS\"\n",
    "_model = \"TCN\"\n",
    "print(_data_set, _model)\n",
    "batch_size = 32\n",
    "\n",
    "hdfs_event_templates = data_sets[\"HDFS\"][\"template_file\"]\n",
    "m = load_event_templates_hdfs(hdfs_event_templates)\n",
    "\n",
    "# load dictionary m into a dataframe, key as column 'EventId', value as column 'EventTemplate'\n",
    "event_templates_df = pd.DataFrame(\n",
    "    m.items(), columns=[\"EventId\", \"EventTemplate\"])\n",
    "\n",
    "word_split = {\n",
    "    \"namesystem\": [\"name\", \"system\"],\n",
    "    \"allocateblock\": [\"allocate\", \"block\"],\n",
    "    \"packetresponder\": [\"packet\", \"responder\"],\n",
    "    \"addstoredblock\": [\"add\", \"stored\", \"block\"],\n",
    "    \"invalidset\": [\"invalid\", \"set\"],\n",
    "    \"ioexception\": [\"input\", \"exception\"],\n",
    "    \"writeblock\": [\"write\", \"block\"],\n",
    "    \"blockinfo\": [\"block\", \"info\"],\n",
    "    \"volumemap\": [\"volume\", \"map\"],\n",
    "    \"receiveblock\": [\"receive\", \"block\"],\n",
    "    \"socketchannel\": [\"socket\", \"channel\"],\n",
    "    \"interruptedioexception\": [\"interrupted\", \"input\", \"exception\"],\n",
    "    \"interruptedinput\": [\"interrupted\", \"input\"],\n",
    "    \"eofexception\": [\"file\", \"exception\"],\n",
    "    \"sockettimeoutexception\": [\"socket\", \"timeout\", \"exception\"],\n",
    "    \"pendingreplicationmonitor\": [\"pending\", \"replication\", \"monitor\"],\n",
    "    \"neededreplications\": [\"needed\", \"replications\"],\n",
    "}\n",
    "\n",
    "event_templates_df = process_text_corpus(event_templates_df, word_split)\n",
    "\n",
    "m = dict(zip(event_templates_df[\"EventId\"],\n",
    "         event_templates_df[\"EventTemplate\"]))\n",
    "\n",
    "x, y = load_data(_data_set, _model)\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "hdfs_corpus_df = pd.DataFrame({\"feature\": x, \"label\": y})\n",
    "hdfs_corpus_df = hdfs_corpus_df.drop(\n",
    "    hdfs_corpus_df[hdfs_corpus_df.label == 0].sample(\n",
    "        frac=0.75, random_state=42).index\n",
    ")\n",
    "hdfs_corpus_df[\"feature_str\"] = (\n",
    "    hdfs_corpus_df[\"feature\"].str.replace(\n",
    "        \"[\", \"\").str.replace(\"]\", \"\").str.split(\",\")\n",
    ")\n",
    "print(hdfs_corpus_df.head())\n",
    "\n",
    "print(\"Apply feature string mapping\")\n",
    "hdfs_corpus_df[\"feature_str\"] = hdfs_corpus_df[\"feature_str\"].apply(\n",
    "    lambda x: [m[str(i)] for i in x]\n",
    ")\n",
    "print(hdfs_corpus_df.head())\n",
    "\n",
    "hdfs_corpus_df[\"feature_str\"] = hdfs_corpus_df[\"feature_str\"].apply(\n",
    "    lambda x: \" \".join(x)\n",
    ")\n",
    "hdfs_corpus_df[\"feature_str\"] = hdfs_corpus_df[\"feature_str\"].apply(\n",
    "    lambda x: x.split(\" \")\n",
    ")\n",
    "train_data = hdfs_corpus_df[\"feature_str\"]\n",
    "train_labels = np.asarray(hdfs_corpus_df[\"label\"].tolist())\n",
    "\n",
    "sequence_length = 128\n",
    "vectorizer = SequenceVectorizer(num_words=sequence_length)\n",
    "vectorizer.fit(train_data)\n",
    "train_data = vectorizer.transform(train_data)\n",
    "vectorizer.tokenizer.word_counts\n",
    "vocab_size = len(vectorizer.tokenizer.word_index) + 1\n",
    "\n",
    "word_index = vectorizer.tokenizer.word_index\n",
    "EMBEDDING_DIM = 64\n",
    "MAX_NB_WORDS = 2000\n",
    "\n",
    "if reload_embedding_index:\n",
    "    path = \"./artefacts/embeddings/\"\n",
    "    pkl_file = f\"{path}/glove.840B.300d.pkl\"\n",
    "    # load embeddings_index from file\n",
    "    with open(pkl_file, \"rb\") as fp:\n",
    "        embeddings_index = pickle.load(fp)\n",
    "\n",
    "print(\"Preparing embedding matrix\")\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
    "word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "dead_words = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # trim the embedding vector to the embedding dimension\n",
    "        if len(embedding_vector) > EMBEDDING_DIM:\n",
    "            embedding_vector = embedding_vector[:EMBEDDING_DIM]\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        dead_words.append(word)\n",
    "print(dead_words)\n",
    "\n",
    "print(\"Null word embeddings: %d\" %\n",
    "      np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "\n",
    "# find words in embedding matrix that nave no embeddings\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    if np.sum(word_embedding_matrix[i]) == 0:\n",
    "        print(word)\n",
    "\n",
    "positional_embedding_matrix = positional_encoding(\n",
    "    len(word_index) + 1, EMBEDDING_DIM)\n",
    "positional_embedding_matrix = positional_embedding_matrix.reshape(\n",
    "    positional_embedding_matrix.shape[1], positional_embedding_matrix.shape[2]\n",
    ")\n",
    "\n",
    "class_counts = np.unique(train_labels, return_counts=True)[1]\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "# print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Normal to anomaly ratio: \", ratio)\n",
    "print(\"Normal: \", class_counts[0])\n",
    "print(\"Anomaly: \", class_counts[1])\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code defines a sentiment classification experiment using a Temporal Convolutional Network (TCN) model.\n",
    "The experiment involves training and evaluating the TCN model on a dataset.\n",
    "\n",
    "Parameters:\n",
    "- embedding_input_dim: The dimension of the input embedding.\n",
    "- input_size: The length of the input sequence.\n",
    "- input_dim: The dimension of the input data.\n",
    "- tcn_filters: The number of filters in the TCN layers.\n",
    "- tcn_kernel_size: The kernel size of the TCN layers.\n",
    "- dropout_rate: The dropout rate for regularization.\n",
    "- num_epochs: The number of training epochs.\n",
    "- num_splits: The number of splits for cross-validation.\n",
    "- prediction_threshold: The threshold for binary classification prediction.\n",
    "\n",
    "Variables:\n",
    "- model_name: The name of the TCN sentiment classifier model.\n",
    "- classifier: The TCN sentiment classifier object.\n",
    "\n",
    "Functions:\n",
    "- cross_validate: Performs cross-validation on the TCN sentiment classifier.\n",
    "- evaluate_model: Evaluates the TCN sentiment classifier model.\n",
    "\n",
    "Note: The code assumes the existence of the TCNSentimentclassifier class and the necessary data and model objects.\n",
    "\"\"\"\n",
    "# embedding_input_dim = 300\n",
    "input_size = sequence_length\n",
    "input_dim = EMBEDDING_DIM\n",
    "tcn_filters = 32\n",
    "tcn_kernel_size = 3\n",
    "dropout_rate = 0.25\n",
    "input_dim = vocab_size\n",
    "num_epochs = 10\n",
    "num_splits = 5\n",
    "prediction_threshold = 0.9\n",
    "\n",
    "model_name = \"TCNSentimentClassifier_HDFS\"\n",
    "\n",
    "classifier = TCNSentimentclassifier(\n",
    "    model_name,\n",
    "    input_size,\n",
    "    input_dim,\n",
    "    positional_embedding_matrix,\n",
    "    word_embedding_matrix,\n",
    "    embedding_output_dim=EMBEDDING_DIM,\n",
    "    tcn_units=128,\n",
    "    prediction_threshold=0.9,\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ") = classifier.cross_validate(\n",
    "    train_data, train_labels, num_splits, num_epochs, prediction_threshold\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    classifier,\n",
    "    _model,\n",
    "    _data_set,\n",
    "    accuracies,\n",
    "    precisions,\n",
    "    recalls,\n",
    "    fscores,\n",
    "    aucs,\n",
    "    conf_matrices,\n",
    "    roc_curves,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the artefacts/models folder\n",
    "!zip -r artefacts.zip ./artefacts/models\n",
    "#zip the output/benchmark_results folder\n",
    "!zip -r output.zip ./output/benchmark_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
